# Анализ производительности MPI-реализации свертки изображений

## Содержание
1.  [Детали реализации](#детали-реализации)
    *   [Инициализация](#1-инициализация)
    *   [Подготовка данных и распределение (Scatter)](#2-подготовка-данных-и-распределение-scatter)
    *   [Локальные вычисления](#3-локальные-вычисления)
    *   [Сбор результатов (Gather)](#4-сбор-результатов-gather)
    *   [Завершение](#5-завершение)
2.  [Введение](#введение)
3.  [Описание тестового "стенда"](#описание-тестового-стенда)
4.  [Основные наблюдения](#основные-наблюдения)
    *   [Общая производительность и масштабируемость](#общая-производительность-и-масштабируемость)
    *   [Сравнение режимов `by_row` и `by_column`](#сравнение-режимов-by_row-и-by_column)
    *   [Влияние "размера блока" (Block Size)](#влияние-размера-блока-block-size)
5. [Анализ производительности используя `perf`](#анализ-используя-perf)
    *   [Сравнение режимов (`by_row` vs `by_column`) при 4 процессах](#Таблица-1:-сравнение-режимов-by_row-и-by_column-при-4-процессах)
    *   [Сравнение производительности от числа процессов](#Таблица-2:-Сравнение-производительности-от-числа-процессов)
    *   [Общие выводы из `perf stat`](#Общие-выводы-из-perf-stat)
6.  [Заключение](#заключение)

### Таблица 2: Сравнение Производительности от Числа Процессов

## Детали реализации
MPI-версия свертки изображения была реализована с использованием библиотеки [MPICH](https://www.mpich.org). Основная идея заключается в разделении (статическом) изображения на части (строки или столбцы), распределении этих частей между MPI-процессами для параллельной обработки и последующем сборе результатов.

Логику работы реализованного подхода можно разбить на несколько фаз:

### 1. Инициализация
*   **Чтение и метаданные:** Нулевой процесс (rank 0) читает входное BMP-изображение (`mpi_rank0_initialize`). Затем размеры изображения (ширина, высота) и длину строки (`row_stride_bytes`) транслируются всем остальным процессам с помощью `MPI_Bcast`.
*   **Расчет распределения:** Каждый процесс вычисляет, за какую часть изображения (диапазон строк или столбцов) он будет отвечать (`mpi_calculate_row_distribution` или `mpi_calculate_column_distribution`). Также определяется необходимый размер `halo_size` - дополнительных строк/столбцов по краям для корректного применения фильтра.

### 2. Подготовка данных и распределение (Scatter)
*   **Подготовка к Scatterv:** Rank 0 вычисляет массивы `sendcounts` (размер данных для каждого процесса в байтах), `displs` (смещение данных для каждого процесса в упакованном буфере) и `origdispls` (исходное смещение данных в *не* упакованном изображении) для операции `MPI_Scatterv`. 
*   **Транспонирование (для `by_column`):** Если выбран режим `BY_COLUMN`, Rank 0 *транспонирует* матрицу пикселей входного изображения. Это позволяет обрабатывать столбцы исходного изображения так, как будто это строки транспонированного, используя тот же механизм распределения `BY_ROW`. Размеры изображения и длина "шага" обновляются соответственно.
*   **Упаковка данных:** Rank 0 создает *непрерывный буфер* , копируя в него необходимые строки (или строки транспонированного изображения) из исходной матрицы пикселей изображения. Это необходимо, так как `MPI_Scatterv` работает с непрерывными буферами памяти. Использование `MPI_Datatype` для работы с 2D-массивами напрямую не было реализовано в данной версии.
*   **Распределение:** Rank 0 рассылает подготовленные куски данных всем процессам (включая себя) с помощью `MPI_Scatterv`. 

### 3. Локальные вычисления
*   **Применение фильтра:** Каждый процесс вызывает функцию `mpi_compute_local_region`, которая, в свою очередь, вызывает `mpi_apply_filter` (или `mpi_apply_median_filter`). Процесс идентичен многопоточной реализации. 

### 4. Сбор результатов (Gather)
*   **Подготовка к Gatherv:** Rank 0 вычисляет массивы `recvcounts` (размер данных, получаемых от каждого процесса) и `recvdispls` (смещение данных от каждого процесса в итоговом буфере на Rank 0) для операции `MPI_Gatherv`.
*   **Сбор:** Все процессы отправляют свои вычисленные результаты из `local_data->output_pixels` на Rank 0 с помощью `MPI_Gatherv`.
*   **Распаковка:** Rank 0 получает данные в буффер и распаковывает их в итоговую 2D-матрицу пикселей `img_data->output_img->img_pixels`. 
*   **Обратное транспонирование (для `by_column`):** Если был выбран режим `BY_COLUMN`, Rank 0 *транспонирует* собранную матрицу пикселей обратно в исходную ориентацию.

### 5. Завершение
*   **Финализация:** Rank 0 останавливает таймер, вычисляет общее время выполнения, сохраняет результирующее изображение и транслирует итоговое время всем процессам с помощью `MPI_Bcast`. Освобождаются ресурсы

[Диаграмма с примерным call-stack](https://github.com/qrutyy/bmp-conv/blob/main/docs/img/MPI-mode-flowchart.png).

### Заметки

Теоритически, с помощью MPI Derived Datatypes можно было создать более сложный тип данных, который описывает разбросанное по 2d массиву расположение строк. Тогда можно было бы передать этот тип данных в MPI_Scatterv (и аналогично для MPI_Gatherv), и MPI-библиотека сама бы (потенциально) собрала/разместила данные из/в нужные места без явной ручной упаковки/распаковки в промежуточный непрерывный буфер.

Однако данный подход требует более детального рассмотрения, и является отнюдь не очевидным. 

Также вы можете задаться вопросом, почему просто не хранить изображение в общей памяти.
Это попросту противоречит стандартной модели MPI.

Еще один подход не подходящий под идею MPI - хранение изображения в памяти каждого узла. Такой подход имеет большую проблему в виде *огромного* использования памяти. Современные изображения, особенно в научных или промышленных приложениях, могут быть очень большими. Тестовое изображение **4480x6720** содержит около 30 миллионов пикселей. Итого: **4480 * 6720 * 3 ≈ 86 МB** только для пиксельных данных. Если у вас `P` MPI-процессов, и каждый хранит полную копию, общее потребление памяти только под данные изображения составит **P * 86 МБ**. При 8 процессах это уже почти 700 МБ. А если изображение гигабайтного размера? Также не мало важным является повышенное использование выч. ресурсов из-за огромного потока данных передающихся через `MPI_Broadcast`. 

## Введение

Тестирование проводилось на изображениях размером **4480 × 6720** ([image6.bmp](https://github.com/qrutyy/bmp-conv/blob/main/test-img/image6.bmp)), с использованием набора процессов от 2 до 8 и 25 повторов для каждого замера. Точная конфигурация сборки указана в [Makefile](https://github.com/qrutyy/bmp-conv/blob/main/Makefile), а скрипт для запуска бенчмарков — в [mpi-benchmark.sh](https://github.com/qrutyy/bmp-conv/blob/main/tests/mpi-benchmark.sh).

В представленные графики не были включены результаты вычислений с по-пиксельным (**by_pixel**) и сеточным (**by_grid**) разбиением изображения. Режим `by_pixel` непрактичен для MPI из-за огромных коммуникационных накладных расходов. Режим `by_grid` не был реализован в данной MPI-версии. "Размер блока", указанный в заголовках графиков, варьировался от 128 до 4.

## Описание тестового "стенда":
Тестирование производилось на системе на базе **Intel Core i7 4790 (x86-64)** - 8 ядер, 3.6 ГГц, ОЗУ 8+ ГБ, GCC 14.2.1. Cистема подвергалась перезагрузке после каждого тестирования, сторонние эффекты были минимизированы + `sudo cpupower frequency-set -g performance`. 

## Основные наблюдения

Анализ предоставленных графиков показывает следующие тенденции производительности MPI-реализации свертки:

### Общая производительность и масштабируемость
*   **Ускорение с ростом числа процессов:** Как и ожидалось, увеличение числа MPI-процессов с 2 до примерно 4-6 приводит к заметному снижению среднего времени выполнения для обоих режимов (`by_row`, `by_column`). Это демонстрирует положительный эффект от распараллеливания вычислений.
*   **Насыщение и спад:** При дальнейшем увеличении числа процессов (до 7-8) ускорение замедляется, а в некоторых случаях наблюдается даже рост времени выполнения. Это может свидетельствовать о классическом проявлении закона Амдала и влияния накладных расходов MPI. С ростом числа процессов увеличивается объем и частота коммуникаций (подготовка данных к Scatterv, сама операция Scatterv, Gatherv), а также время на синхронизацию. В какой-то момент эти накладные расходы начинают преобладать над выгодой от уменьшения объема вычислений на один процесс.

### Сравнение режимов `by_row` и `by_column`
*   **Относительная производительность:** На всех представленных графиках режим `by_row` показывает лучшую производительность по сравнению с `by_column`. Это обусловлено отсутствием дополнительной логики для транспонирования матрицы и ее обработки в случае `by_column`.
*   **Анализ:** Режим `by_column` требует дополнительных операций транспонирования матрицы на Rank 0 (до Scatter и после Gather). Это добавляет последовательные накладные расходы. 

### Влияние "размера блока" (Block Size)
**Параметр не влияет на распределение MPI:** "Block Size" в данном контексте не определяет размер кусков данных, передаваемых через `MPI_Scatterv`/`MPI_Gatherv`. Распределение (`my_num_rc`, `send_num_rc`) рассчитывается на основе общего числа строк/столбцов и числа процессов, а не этого "Block Size".
 

## Анализ используя perf
В дополнение к анализу времени выполнения, были собраны данные счетчиков производительности процессора с помощью утилиты `perf stat` для более глубокого понимания узких мест и эффективности использования аппаратных ресурсов.

### Таблица 1: Сравнение режимов by_row и by_column при 4 процессах

| Метрика                     | `by_column` (np=4, gg, b=5) | `by_row` (np=4, gg, b=5) | Анализ                                                                                                |
| :-------------------------- | :-------------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------- |
| **Время выполнения (с)**    | **5.904**                   | **5.335**                | `by_row` **быстрее** на ~10% для этой конфигурации.                                                   |
| CPU Utilized                | 3.815                       | 3.795                    | Очень близкая, хорошая утилизация CPU (близко к 4.0).                                                 |
| Инструкций на Цикл (IPC)    | 3.32                        | 3.47                     | `by_row` немного эффективнее на микроархитектурном уровне (меньше простоев на цикл).               |
| **L1d Cache Load Miss %**   | 0.31%                       | **0.17%**                | `by_row` имеет **значительно меньше** промахов L1 кэша данных.          |
| **LLC (L3) Load Miss %**    | **14.97%**                  | 25.54%                   | `by_column` имеет **существенно меньше** промахов L3 кэша.   |
| Branch Misses %             | 0.15%                       | 0.16%                    | Очень низкий и схожий процент промахов ветвлений. Не является решающим фактором.                   |
| Instructions (млрд)         | ~282.7                      | ~265.8                   | `by_row` выполняет меньше инструкций (из-за отсутствия инструкций транспонирования).       |
| Cycles (млрд)               | ~85.2                       | ~76.6                    | Меньше циклов у `by_row`.                                                                             |
| Context Switches (/sec)     | ~219                        | ~244                     | Схожее количество переключений контекста.                                                             |


**Анализ Сравнения Режимов:**

1.  **Время Выполнения:** Для данной конфигурации `by_row` оказался быстрее `by_column`. Последовательные накладные расходы на **транспонирование** в `by_column` перевесили выигрыш от лучшей L3-локальности на этапе вычислений.
2.  **Кэш L1 vs L3:** Наблюдается компромисс:
    *   `by_row` показывает отличную **L1-локальность** (0.17% промахов), что указывает на эффективный последовательный доступ к данным во время локальной обработки строк.
    *   `by_column` выигрывает по **L3-локальности** (15% промахов против 25.5%), что означает, что рабочий набор данных (транспонированный блок) лучше удерживается в большом кэше L3. Более высокий процент промахов L1 (0.31%) может быть связан как с локальными вычислениями, так и с операциями транспонирования/упаковки.
    Победа `by_row` по времени говорит о том, что при 4 процессах **эффективность L1 и отсутствие затрат на транспонирование оказались важнее** оптимизации L3.
3.  **IPC:** Небольшое преимущество `by_row` по IPC (3.47 vs 3.32) коррелирует с лучшей производительностью L1 кэша (меньше ожиданий данных).

---

### Таблица 2: Сравнение производительности от числа процессов

| PROC (np) | TIME (S)        | CPU UTIL | IPC    | L1D MISS % | L3 MISS % | BRANCH MISS % |                                  
| :-------- | :--------------- | :------- | :----- | :--------- | :--------- |  :----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2         | 9.316            | 1.946    | 3.49   | 0.16%      | 24.26%     | 0.16%         | 
| 3         | 6.669 (-28%)     | 2.878    | 3.47   | 0.16%      | 24.72%     | 0.16%         |                                                                                                    
| 4         | **5.335 (-20%)** | 3.795    | 3.47   | 0.17%      | 25.54%     | 0.16%         | 
| 5         | 7.839 (+47%)     | 4.830    | **2.78** | 0.12%      | 20.64%     | 0.13%          
| 6         | 6.734 (-14%)     | 5.767    | **2.29** | 0.17%      | 19.08%     | 0.19%         |           
| 7         | 5.989 (-11%)     | 6.715    | **1.91** | 0.23%      | 18.98%     | 0.26%         |                                                                          
| 8         | 5.426 (-9%)      | 7.656    | **1.74** | 0.29%      | 17.68%     | 0.32%         |                                                     

**Анализ масштабируемости (режим `by_row`, gg):**

1.  **Оптимальное Число Процессов:** Для данной задачи, фильтра и системы **оптимальным является 4 процесса**. Дальнейшее увеличение числа процессов приводит к падению производительности (np=5) или очень незначительному улучшению (np=6-8) ценой резкого снижения эффективности.
2.  **Падение Эффективности (IPC):** Самый яркий показатель — **падение IPC** с ~3.5 до 1.7 при переходе от 2 к 8 процессам. Это означает, что процессоры все больше времени проводят в ожидании (данных из памяти, сообщений MPI, синхронизации), а не на выполнении полезных инструкций.
3.  **Коммуникационные Затраты:** С ростом числа процессов увеличивается объем и частота MPI-коммуникаций (`Scatterv`, `Gatherv`, `Bcast`), а также накладные расходы на последовательные операции на Rank 0 (упаковка/распаковка, расчеты `displs` и т.д.). Эти издержки начинают доминировать над выгодой от распараллеливания вычислений.
4.  **Поведение Кэшей:**
    *   **L1d:** Промахи остаются низкими (<0.3%), но немного растут при 7-8 процессах. Возможно, это связано с тем, что чанки данных становятся меньше, и работа с границами (гало) занимает относительно большую долю времени.
    *   **LLC (L3):** Процент промахов L3 *уменьшается* при увеличении числа процессов с 4 до 8 (с 25% до 17%). Это логично: рабочий набор данных *каждого* процесса становится меньше и лучше помещается в L3. Однако, это улучшение локальности L3 **не компенсирует** рост коммуникационных издержек и общее падение эффективности (IPC).
5.  **Аномалия при np=5:** Резкое ухудшение времени и IPC при 5 процессах требует дополнительного исследования. Это может быть связано с неэффективным распределением нагрузки или проблемами планировщика.


### Общие выводы из perf stat

*    Падение IPC является главным индикатором того, что накладные расходы (коммуникации, синхронизация, последовательные части) начинают доминировать над вычислениями.
*   Эффективность L1 и L3 кэшей важна, но не является единственным фактором. Для np=4 лучшая L1-локальность `by_row` перевесила лучшую L3-локальность `by_column`.
*   Для данной реализации и системы предел эффективной масштабируемости достигается примерно на 4 процессах. Добавление большего числа процессоров непродуктивно из-за архитектуры MPI-приложения (значительные коммуникационные и последовательные компоненты).
*   Основные усилия по оптимизации следует направить на снижение коммуникационных издержек (например, через `MPI_Datatype` или неблокирующие операции) и минимизацию последовательной работы на Rank 0.


## Заключение

Проведенный анализ производительности MPI-реализации свертки изображений выявил следующее:

*   **Масштабируемость ограничена:** Реализация демонстрирует ускорение при увеличении числа процессов до 4-6, после чего производительность стабилизируется или незначительно ухудшается из-за роста коммуникационных накладных расходов, свойственных MPI.
*   **Режим `by_column` немного эффективнее:** Несмотря на дополнительные затраты на транспонирование на Rank 0, режим распределения по столбцам (`by_column`) показывает стабильно немного лучшие результаты, чем распределение по строкам (`by_row`), особенно при большем числе процессов. Это может быть связано с более эффективным доступом к данным во время локальных вычислений на транспонированных блоках.
*   **Коммуникации - основной барьер:** Основным ограничителем производительности при большом числе процессов, вероятно, являются затраты на передачу данных (подготовка буферов, операции `MPI_Scatterv`/`MPI_Gatherv`) и синхронизацию между процессами. Оптимизация этих аспектов (например, использование неблокирующих коммуникаций, более эффективных типов данных MPI) может дать дальнейший прирост производительности.


